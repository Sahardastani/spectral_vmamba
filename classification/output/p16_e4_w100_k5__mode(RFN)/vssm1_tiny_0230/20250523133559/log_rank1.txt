[2025-05-23 13:36:00 vssm1_tiny_0230] (main.py 447): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /data/shared/mini-imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  MMCKPT: false
  NAME: vssm1_tiny_0230
  NUM_CLASSES: 100
  PRETRAINED: ''
  RESUME: ''
  TYPE: vssm
  VSSM:
    ALPHA: 100
    AMBIGUITY: false
    BINARY: false
    CSMS6S_MODE: NORMAL
    DEPTHS:
    - 2
    - 2
    - 5
    - 2
    DIMENSION:
    - INCREASE
    DIVISION_RATE: 4
    DOWNSAMPLE: v3
    EMBED_DIM: 256
    GMLP: false
    IN_CHANS: 3
    KNN: 5
    K_GROUP: 8
    MLP_ACT_LAYER: gelu
    MLP_DROP_RATE: 0.0
    MLP_RATIO: 4.0
    MODE: RFN
    NORM_LAYER: ln2d
    PATCHEMBED: v2
    PATCH_NORM: true
    PATCH_SIZE: 16
    POSEMBED: false
    SSM_ACT_LAYER: silu
    SSM_CONV: 1
    SSM_CONV_BIAS: false
    SSM_DROP_RATE: 0.0
    SSM_DT_RANK: auto
    SSM_D_STATE: 1
    SSM_FORWARDTYPE: v05_noz
    SSM_INIT: v0
    SSM_RANK_RATIO: 2.0
    SSM_RATIO: 2.0
    TOP_K: 4
OUTPUT: output/p16_e4_w100_k5__mode(RFN)/vssm1_tiny_0230/20250523133559
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: '20250523133559'
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 0.00025
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 2.5e-06
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 0.05
TRAINCOST_MODE: false

[2025-05-23 13:36:00 vssm1_tiny_0230] (main.py 448): INFO {"cfg": "configs/vssm/vmambav2_tiny_224.yaml", "opts": null, "batch_size": 128, "data_path": "/data/shared/mini-imagenet", "zip": false, "cache_mode": "part", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "output": "output", "tag": "20250523133559", "eval": false, "throughput": false, "fused_layernorm": false, "optim": null, "model_ema": true, "model_ema_decay": 0.9999, "model_ema_force_cpu": false, "memory_limit_rate": -1, "top_k": 2, "knn": 4}
[2025-05-23 13:36:00 vssm1_tiny_0230] (main.py 128): INFO Creating model:vssm/vssm1_tiny_0230
[2025-05-23 13:36:03 vssm1_tiny_0230] (optimizer.py 18): INFO ==============> building optimizer adamw....................
[2025-05-23 13:36:03 vssm1_tiny_0230] (optimizer.py 26): INFO No weight decay list: ['layers.0.blocks.0.norm.weight', 'layers.0.blocks.0.norm.bias', 'layers.0.blocks.0.op.Ds', 'layers.0.blocks.0.op.out_norm.weight', 'layers.0.blocks.0.op.out_norm.bias', 'layers.0.blocks.0.norm2.weight', 'layers.0.blocks.0.norm2.bias', 'layers.0.blocks.0.mlp.fc1.bias', 'layers.0.blocks.0.mlp.fc2.bias', 'layers.0.blocks.1.norm.weight', 'layers.0.blocks.1.norm.bias', 'layers.0.blocks.1.op.Ds', 'layers.0.blocks.1.op.out_norm.weight', 'layers.0.blocks.1.op.out_norm.bias', 'layers.0.blocks.1.norm2.weight', 'layers.0.blocks.1.norm2.bias', 'layers.0.blocks.1.mlp.fc1.bias', 'layers.0.blocks.1.mlp.fc2.bias', 'layers.0.downsample.1.bias', 'layers.0.downsample.4.weight', 'layers.0.downsample.4.bias', 'layers.1.blocks.0.norm.weight', 'layers.1.blocks.0.norm.bias', 'layers.1.blocks.0.op.Ds', 'layers.1.blocks.0.op.out_norm.weight', 'layers.1.blocks.0.op.out_norm.bias', 'layers.1.blocks.0.norm2.weight', 'layers.1.blocks.0.norm2.bias', 'layers.1.blocks.0.mlp.fc1.bias', 'layers.1.blocks.0.mlp.fc2.bias', 'layers.1.blocks.1.norm.weight', 'layers.1.blocks.1.norm.bias', 'layers.1.blocks.1.op.Ds', 'layers.1.blocks.1.op.out_norm.weight', 'layers.1.blocks.1.op.out_norm.bias', 'layers.1.blocks.1.norm2.weight', 'layers.1.blocks.1.norm2.bias', 'layers.1.blocks.1.mlp.fc1.bias', 'layers.1.blocks.1.mlp.fc2.bias', 'layers.1.downsample.1.bias', 'layers.1.downsample.4.weight', 'layers.1.downsample.4.bias', 'layers.2.blocks.0.norm.weight', 'layers.2.blocks.0.norm.bias', 'layers.2.blocks.0.op.Ds', 'layers.2.blocks.0.op.out_norm.weight', 'layers.2.blocks.0.op.out_norm.bias', 'layers.2.blocks.0.norm2.weight', 'layers.2.blocks.0.norm2.bias', 'layers.2.blocks.0.mlp.fc1.bias', 'layers.2.blocks.0.mlp.fc2.bias', 'layers.2.blocks.1.norm.weight', 'layers.2.blocks.1.norm.bias', 'layers.2.blocks.1.op.Ds', 'layers.2.blocks.1.op.out_norm.weight', 'layers.2.blocks.1.op.out_norm.bias', 'layers.2.blocks.1.norm2.weight', 'layers.2.blocks.1.norm2.bias', 'layers.2.blocks.1.mlp.fc1.bias', 'layers.2.blocks.1.mlp.fc2.bias', 'layers.2.blocks.2.norm.weight', 'layers.2.blocks.2.norm.bias', 'layers.2.blocks.2.op.Ds', 'layers.2.blocks.2.op.out_norm.weight', 'layers.2.blocks.2.op.out_norm.bias', 'layers.2.blocks.2.norm2.weight', 'layers.2.blocks.2.norm2.bias', 'layers.2.blocks.2.mlp.fc1.bias', 'layers.2.blocks.2.mlp.fc2.bias', 'layers.2.blocks.3.norm.weight', 'layers.2.blocks.3.norm.bias', 'layers.2.blocks.3.op.Ds', 'layers.2.blocks.3.op.out_norm.weight', 'layers.2.blocks.3.op.out_norm.bias', 'layers.2.blocks.3.norm2.weight', 'layers.2.blocks.3.norm2.bias', 'layers.2.blocks.3.mlp.fc1.bias', 'layers.2.blocks.3.mlp.fc2.bias', 'layers.2.blocks.4.norm.weight', 'layers.2.blocks.4.norm.bias', 'layers.2.blocks.4.op.Ds', 'layers.2.blocks.4.op.out_norm.weight', 'layers.2.blocks.4.op.out_norm.bias', 'layers.2.blocks.4.norm2.weight', 'layers.2.blocks.4.norm2.bias', 'layers.2.blocks.4.mlp.fc1.bias', 'layers.2.blocks.4.mlp.fc2.bias', 'layers.2.downsample.1.bias', 'layers.2.downsample.4.weight', 'layers.2.downsample.4.bias', 'layers.3.blocks.0.norm.weight', 'layers.3.blocks.0.norm.bias', 'layers.3.blocks.0.op.Ds', 'layers.3.blocks.0.op.out_norm.weight', 'layers.3.blocks.0.op.out_norm.bias', 'layers.3.blocks.0.norm2.weight', 'layers.3.blocks.0.norm2.bias', 'layers.3.blocks.0.mlp.fc1.bias', 'layers.3.blocks.0.mlp.fc2.bias', 'layers.3.blocks.1.norm.weight', 'layers.3.blocks.1.norm.bias', 'layers.3.blocks.1.op.Ds', 'layers.3.blocks.1.op.out_norm.weight', 'layers.3.blocks.1.op.out_norm.bias', 'layers.3.blocks.1.norm2.weight', 'layers.3.blocks.1.norm2.bias', 'layers.3.blocks.1.mlp.fc1.bias', 'layers.3.blocks.1.mlp.fc2.bias', 'classifier.norm.weight', 'classifier.norm.bias', 'classifier.head.bias']
[2025-05-23 13:36:03 vssm1_tiny_0230] (main.py 177): INFO no checkpoint found in output/p16_e4_w100_k5__mode(RFN)/vssm1_tiny_0230/20250523133559, ignoring auto resume
[2025-05-23 13:36:03 vssm1_tiny_0230] (main.py 210): INFO Start training
[2025-05-23 13:36:36 vssm1_tiny_0230] (main.py 299): INFO Train: [0/300][0/195]	eta 1:47:09 lr 0.000000	 wd 0.0500	time 32.9699 (32.9699)	data time 25.9439 (25.9439)	model time 0.0000 (0.0000)	loss 7.1280 (7.1280)	grad_norm 7.2470 (7.2470)	loss_scale 65536.0000 (65536.0000)	mem 41350MB
[2025-05-23 13:36:51 vssm1_tiny_0230] (main.py 299): INFO Train: [0/300][10/195]	eta 0:13:28 lr 0.000001	 wd 0.0500	time 1.4886 (4.3718)	data time 0.0006 (2.3591)	model time 0.0000 (0.0000)	loss 7.1313 (7.0904)	grad_norm 6.8703 (6.9436)	loss_scale 65536.0000 (65536.0000)	mem 12650MB
[2025-05-23 13:37:06 vssm1_tiny_0230] (main.py 299): INFO Train: [0/300][20/195]	eta 0:08:45 lr 0.000002	 wd 0.0500	time 1.3493 (3.0003)	data time 0.0007 (1.2361)	model time 0.0000 (0.0000)	loss 6.8593 (7.0307)	grad_norm 6.7239 (6.8976)	loss_scale 65536.0000 (65536.0000)	mem 12650MB
[2025-05-23 13:37:21 vssm1_tiny_0230] (main.py 299): INFO Train: [0/300][30/195]	eta 0:06:56 lr 0.000002	 wd 0.0500	time 1.7325 (2.5237)	data time 0.0006 (0.8376)	model time 0.0000 (0.0000)	loss 6.5502 (6.9363)	grad_norm 6.8370 (6.8099)	loss_scale 65536.0000 (65536.0000)	mem 12650MB
[2025-05-23 13:37:36 vssm1_tiny_0230] (main.py 299): INFO Train: [0/300][40/195]	eta 0:05:51 lr 0.000003	 wd 0.0500	time 1.3986 (2.2696)	data time 0.0006 (0.6335)	model time 0.0000 (0.0000)	loss 6.2880 (6.8118)	grad_norm 6.3537 (6.7534)	loss_scale 65536.0000 (65536.0000)	mem 12650MB
[2025-05-23 13:37:51 vssm1_tiny_0230] (main.py 299): INFO Train: [0/300][50/195]	eta 0:05:06 lr 0.000003	 wd 0.0500	time 1.3429 (2.1155)	data time 0.0007 (0.5094)	model time 0.0000 (0.0000)	loss 5.9587 (6.6614)	grad_norm 6.3464 (6.6290)	loss_scale 65536.0000 (65536.0000)	mem 12650MB
[2025-05-23 13:38:06 vssm1_tiny_0230] (main.py 299): INFO Train: [0/300][60/195]	eta 0:04:32 lr 0.000004	 wd 0.0500	time 1.7100 (2.0171)	data time 0.0008 (0.4260)	model time 1.7092 (1.5148)	loss 5.5601 (6.5068)	grad_norm 5.8251 (6.4872)	loss_scale 65536.0000 (65536.0000)	mem 12650MB
[2025-05-23 13:38:21 vssm1_tiny_0230] (main.py 299): INFO Train: [0/300][70/195]	eta 0:04:02 lr 0.000005	 wd 0.0500	time 1.3770 (1.9430)	data time 0.0008 (0.3661)	model time 1.3762 (1.5023)	loss 5.3917 (6.3631)	grad_norm 4.5945 (6.3174)	loss_scale 65536.0000 (65536.0000)	mem 12650MB
[2025-05-23 13:38:36 vssm1_tiny_0230] (main.py 299): INFO Train: [0/300][80/195]	eta 0:03:37 lr 0.000005	 wd 0.0500	time 1.4231 (1.8877)	data time 0.0007 (0.3210)	model time 1.4224 (1.4997)	loss 5.3747 (6.2385)	grad_norm 4.9948 (6.1421)	loss_scale 65536.0000 (65536.0000)	mem 12650MB
[2025-05-23 13:38:51 vssm1_tiny_0230] (main.py 299): INFO Train: [0/300][90/195]	eta 0:03:13 lr 0.000006	 wd 0.0500	time 1.6468 (1.8469)	data time 0.0008 (0.2858)	model time 1.6460 (1.5037)	loss 5.2242 (6.1317)	grad_norm 4.5164 (5.9715)	loss_scale 65536.0000 (65536.0000)	mem 12650MB
[2025-05-23 13:39:06 vssm1_tiny_0230] (main.py 299): INFO Train: [0/300][100/195]	eta 0:02:52 lr 0.000007	 wd 0.0500	time 1.3110 (1.8109)	data time 0.0008 (0.2576)	model time 1.3102 (1.4994)	loss 5.1880 (6.0384)	grad_norm 4.1282 (5.8317)	loss_scale 65536.0000 (65536.0000)	mem 12650MB
[2025-05-23 13:39:21 vssm1_tiny_0230] (main.py 299): INFO Train: [0/300][110/195]	eta 0:02:31 lr 0.000007	 wd 0.0500	time 1.6448 (1.7850)	data time 0.0008 (0.2345)	model time 1.6441 (1.5034)	loss 5.1876 (5.9585)	grad_norm 4.4259 (5.7092)	loss_scale 65536.0000 (65536.0000)	mem 12650MB
[2025-05-23 13:39:36 vssm1_tiny_0230] (main.py 299): INFO Train: [0/300][120/195]	eta 0:02:12 lr 0.000008	 wd 0.0500	time 1.4995 (1.7615)	data time 0.0006 (0.2151)	model time 1.4989 (1.5027)	loss 5.1108 (5.8897)	grad_norm 4.5020 (5.5937)	loss_scale 65536.0000 (65536.0000)	mem 12650MB
[2025-05-23 13:39:51 vssm1_tiny_0230] (main.py 299): INFO Train: [0/300][130/195]	eta 0:01:53 lr 0.000009	 wd 0.0500	time 1.2721 (1.7400)	data time 0.0006 (0.1988)	model time 1.2715 (1.4999)	loss 5.0829 (5.8283)	grad_norm 3.8647 (5.4944)	loss_scale 65536.0000 (65536.0000)	mem 12650MB
[2025-05-23 13:40:03 vssm1_tiny_0230] (main.py 299): INFO Train: [0/300][140/195]	eta 0:01:33 lr 0.000009	 wd 0.0500	time 1.5528 (1.7016)	data time 0.0006 (0.1847)	model time 1.5522 (1.4663)	loss 5.0641 (5.7748)	grad_norm 4.8419 (5.4127)	loss_scale 65536.0000 (65536.0000)	mem 12650MB
[2025-05-23 13:40:18 vssm1_tiny_0230] (main.py 299): INFO Train: [0/300][150/195]	eta 0:01:15 lr 0.000010	 wd 0.0500	time 1.2920 (1.6870)	data time 0.0008 (0.1725)	model time 1.2912 (1.4677)	loss 5.1342 (5.7263)	grad_norm 4.8075 (5.3474)	loss_scale 65536.0000 (65536.0000)	mem 12650MB
