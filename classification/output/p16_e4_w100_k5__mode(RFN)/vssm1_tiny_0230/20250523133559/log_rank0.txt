[2025-05-23 13:36:00 vssm1_tiny_0230] (main.py 444): INFO Full config saved to output/p16_e4_w100_k5__mode(RFN)/vssm1_tiny_0230/20250523133559/config.json
[2025-05-23 13:36:00 vssm1_tiny_0230] (main.py 447): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /data/shared/mini-imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  MMCKPT: false
  NAME: vssm1_tiny_0230
  NUM_CLASSES: 100
  PRETRAINED: ''
  RESUME: ''
  TYPE: vssm
  VSSM:
    ALPHA: 100
    AMBIGUITY: false
    BINARY: false
    CSMS6S_MODE: NORMAL
    DEPTHS:
    - 2
    - 2
    - 5
    - 2
    DIMENSION:
    - INCREASE
    DIVISION_RATE: 4
    DOWNSAMPLE: v3
    EMBED_DIM: 256
    GMLP: false
    IN_CHANS: 3
    KNN: 5
    K_GROUP: 8
    MLP_ACT_LAYER: gelu
    MLP_DROP_RATE: 0.0
    MLP_RATIO: 4.0
    MODE: RFN
    NORM_LAYER: ln2d
    PATCHEMBED: v2
    PATCH_NORM: true
    PATCH_SIZE: 16
    POSEMBED: false
    SSM_ACT_LAYER: silu
    SSM_CONV: 1
    SSM_CONV_BIAS: false
    SSM_DROP_RATE: 0.0
    SSM_DT_RANK: auto
    SSM_D_STATE: 1
    SSM_FORWARDTYPE: v05_noz
    SSM_INIT: v0
    SSM_RANK_RATIO: 2.0
    SSM_RATIO: 2.0
    TOP_K: 4
OUTPUT: output/p16_e4_w100_k5__mode(RFN)/vssm1_tiny_0230/20250523133559
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: '20250523133559'
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 0.00025
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 2.5e-06
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 0.05
TRAINCOST_MODE: false

[2025-05-23 13:36:00 vssm1_tiny_0230] (main.py 448): INFO {"cfg": "configs/vssm/vmambav2_tiny_224.yaml", "opts": null, "batch_size": 128, "data_path": "/data/shared/mini-imagenet", "zip": false, "cache_mode": "part", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "output": "output", "tag": "20250523133559", "eval": false, "throughput": false, "fused_layernorm": false, "optim": null, "model_ema": true, "model_ema_decay": 0.9999, "model_ema_force_cpu": false, "memory_limit_rate": -1, "top_k": 2, "knn": 4}
[2025-05-23 13:36:00 vssm1_tiny_0230] (main.py 128): INFO Creating model:vssm/vssm1_tiny_0230
[2025-05-23 13:36:03 vssm1_tiny_0230] (optimizer.py 18): INFO ==============> building optimizer adamw....................
[2025-05-23 13:36:03 vssm1_tiny_0230] (optimizer.py 26): INFO No weight decay list: ['layers.0.blocks.0.norm.weight', 'layers.0.blocks.0.norm.bias', 'layers.0.blocks.0.op.Ds', 'layers.0.blocks.0.op.out_norm.weight', 'layers.0.blocks.0.op.out_norm.bias', 'layers.0.blocks.0.norm2.weight', 'layers.0.blocks.0.norm2.bias', 'layers.0.blocks.0.mlp.fc1.bias', 'layers.0.blocks.0.mlp.fc2.bias', 'layers.0.blocks.1.norm.weight', 'layers.0.blocks.1.norm.bias', 'layers.0.blocks.1.op.Ds', 'layers.0.blocks.1.op.out_norm.weight', 'layers.0.blocks.1.op.out_norm.bias', 'layers.0.blocks.1.norm2.weight', 'layers.0.blocks.1.norm2.bias', 'layers.0.blocks.1.mlp.fc1.bias', 'layers.0.blocks.1.mlp.fc2.bias', 'layers.0.downsample.1.bias', 'layers.0.downsample.4.weight', 'layers.0.downsample.4.bias', 'layers.1.blocks.0.norm.weight', 'layers.1.blocks.0.norm.bias', 'layers.1.blocks.0.op.Ds', 'layers.1.blocks.0.op.out_norm.weight', 'layers.1.blocks.0.op.out_norm.bias', 'layers.1.blocks.0.norm2.weight', 'layers.1.blocks.0.norm2.bias', 'layers.1.blocks.0.mlp.fc1.bias', 'layers.1.blocks.0.mlp.fc2.bias', 'layers.1.blocks.1.norm.weight', 'layers.1.blocks.1.norm.bias', 'layers.1.blocks.1.op.Ds', 'layers.1.blocks.1.op.out_norm.weight', 'layers.1.blocks.1.op.out_norm.bias', 'layers.1.blocks.1.norm2.weight', 'layers.1.blocks.1.norm2.bias', 'layers.1.blocks.1.mlp.fc1.bias', 'layers.1.blocks.1.mlp.fc2.bias', 'layers.1.downsample.1.bias', 'layers.1.downsample.4.weight', 'layers.1.downsample.4.bias', 'layers.2.blocks.0.norm.weight', 'layers.2.blocks.0.norm.bias', 'layers.2.blocks.0.op.Ds', 'layers.2.blocks.0.op.out_norm.weight', 'layers.2.blocks.0.op.out_norm.bias', 'layers.2.blocks.0.norm2.weight', 'layers.2.blocks.0.norm2.bias', 'layers.2.blocks.0.mlp.fc1.bias', 'layers.2.blocks.0.mlp.fc2.bias', 'layers.2.blocks.1.norm.weight', 'layers.2.blocks.1.norm.bias', 'layers.2.blocks.1.op.Ds', 'layers.2.blocks.1.op.out_norm.weight', 'layers.2.blocks.1.op.out_norm.bias', 'layers.2.blocks.1.norm2.weight', 'layers.2.blocks.1.norm2.bias', 'layers.2.blocks.1.mlp.fc1.bias', 'layers.2.blocks.1.mlp.fc2.bias', 'layers.2.blocks.2.norm.weight', 'layers.2.blocks.2.norm.bias', 'layers.2.blocks.2.op.Ds', 'layers.2.blocks.2.op.out_norm.weight', 'layers.2.blocks.2.op.out_norm.bias', 'layers.2.blocks.2.norm2.weight', 'layers.2.blocks.2.norm2.bias', 'layers.2.blocks.2.mlp.fc1.bias', 'layers.2.blocks.2.mlp.fc2.bias', 'layers.2.blocks.3.norm.weight', 'layers.2.blocks.3.norm.bias', 'layers.2.blocks.3.op.Ds', 'layers.2.blocks.3.op.out_norm.weight', 'layers.2.blocks.3.op.out_norm.bias', 'layers.2.blocks.3.norm2.weight', 'layers.2.blocks.3.norm2.bias', 'layers.2.blocks.3.mlp.fc1.bias', 'layers.2.blocks.3.mlp.fc2.bias', 'layers.2.blocks.4.norm.weight', 'layers.2.blocks.4.norm.bias', 'layers.2.blocks.4.op.Ds', 'layers.2.blocks.4.op.out_norm.weight', 'layers.2.blocks.4.op.out_norm.bias', 'layers.2.blocks.4.norm2.weight', 'layers.2.blocks.4.norm2.bias', 'layers.2.blocks.4.mlp.fc1.bias', 'layers.2.blocks.4.mlp.fc2.bias', 'layers.2.downsample.1.bias', 'layers.2.downsample.4.weight', 'layers.2.downsample.4.bias', 'layers.3.blocks.0.norm.weight', 'layers.3.blocks.0.norm.bias', 'layers.3.blocks.0.op.Ds', 'layers.3.blocks.0.op.out_norm.weight', 'layers.3.blocks.0.op.out_norm.bias', 'layers.3.blocks.0.norm2.weight', 'layers.3.blocks.0.norm2.bias', 'layers.3.blocks.0.mlp.fc1.bias', 'layers.3.blocks.0.mlp.fc2.bias', 'layers.3.blocks.1.norm.weight', 'layers.3.blocks.1.norm.bias', 'layers.3.blocks.1.op.Ds', 'layers.3.blocks.1.op.out_norm.weight', 'layers.3.blocks.1.op.out_norm.bias', 'layers.3.blocks.1.norm2.weight', 'layers.3.blocks.1.norm2.bias', 'layers.3.blocks.1.mlp.fc1.bias', 'layers.3.blocks.1.mlp.fc2.bias', 'classifier.norm.weight', 'classifier.norm.bias', 'classifier.head.bias']
[2025-05-23 13:36:03 vssm1_tiny_0230] (main.py 177): INFO no checkpoint found in output/p16_e4_w100_k5__mode(RFN)/vssm1_tiny_0230/20250523133559, ignoring auto resume
[2025-05-23 13:36:03 vssm1_tiny_0230] (main.py 210): INFO Start training
[2025-05-23 13:36:36 vssm1_tiny_0230] (main.py 299): INFO Train: [0/300][0/195]	eta 1:47:12 lr 0.000000	 wd 0.0500	time 32.9883 (32.9883)	data time 26.2024 (26.2024)	model time 0.0000 (0.0000)	loss 7.0823 (7.0823)	grad_norm 7.2470 (7.2470)	loss_scale 65536.0000 (65536.0000)	mem 19967MB
[2025-05-23 13:36:51 vssm1_tiny_0230] (main.py 299): INFO Train: [0/300][10/195]	eta 0:13:28 lr 0.000001	 wd 0.0500	time 1.4812 (4.3726)	data time 0.0007 (2.3826)	model time 0.0000 (0.0000)	loss 6.9982 (7.0630)	grad_norm 6.8703 (6.9436)	loss_scale 65536.0000 (65536.0000)	mem 12648MB
[2025-05-23 13:37:06 vssm1_tiny_0230] (main.py 299): INFO Train: [0/300][20/195]	eta 0:08:45 lr 0.000002	 wd 0.0500	time 1.3555 (3.0010)	data time 0.0007 (1.2484)	model time 0.0000 (0.0000)	loss 6.8514 (7.0155)	grad_norm 6.7239 (6.8976)	loss_scale 65536.0000 (65536.0000)	mem 12648MB
[2025-05-23 13:37:21 vssm1_tiny_0230] (main.py 299): INFO Train: [0/300][30/195]	eta 0:06:56 lr 0.000002	 wd 0.0500	time 1.7254 (2.5241)	data time 0.0005 (0.8459)	model time 0.0000 (0.0000)	loss 6.5344 (6.9228)	grad_norm 6.8370 (6.8099)	loss_scale 65536.0000 (65536.0000)	mem 12648MB
[2025-05-23 13:37:36 vssm1_tiny_0230] (main.py 299): INFO Train: [0/300][40/195]	eta 0:05:51 lr 0.000003	 wd 0.0500	time 1.3900 (2.2698)	data time 0.0007 (0.6398)	model time 0.0000 (0.0000)	loss 6.2219 (6.7959)	grad_norm 6.3537 (6.7534)	loss_scale 65536.0000 (65536.0000)	mem 12648MB
[2025-05-23 13:37:51 vssm1_tiny_0230] (main.py 299): INFO Train: [0/300][50/195]	eta 0:05:06 lr 0.000003	 wd 0.0500	time 1.3506 (2.1158)	data time 0.0006 (0.5145)	model time 0.0000 (0.0000)	loss 5.8683 (6.6524)	grad_norm 6.3464 (6.6290)	loss_scale 65536.0000 (65536.0000)	mem 12648MB
[2025-05-23 13:38:06 vssm1_tiny_0230] (main.py 299): INFO Train: [0/300][60/195]	eta 0:04:32 lr 0.000004	 wd 0.0500	time 1.7192 (2.0174)	data time 0.0009 (0.4302)	model time 1.7182 (1.5146)	loss 5.6407 (6.5053)	grad_norm 5.8251 (6.4872)	loss_scale 65536.0000 (65536.0000)	mem 12648MB
[2025-05-23 13:38:21 vssm1_tiny_0230] (main.py 299): INFO Train: [0/300][70/195]	eta 0:04:02 lr 0.000005	 wd 0.0500	time 1.3699 (1.9431)	data time 0.0007 (0.3697)	model time 1.3691 (1.5020)	loss 5.3773 (6.3636)	grad_norm 4.5945 (6.3174)	loss_scale 65536.0000 (65536.0000)	mem 12648MB
[2025-05-23 13:38:36 vssm1_tiny_0230] (main.py 299): INFO Train: [0/300][80/195]	eta 0:03:37 lr 0.000005	 wd 0.0500	time 1.4318 (1.8879)	data time 0.0007 (0.3242)	model time 1.4311 (1.4997)	loss 5.2774 (6.2385)	grad_norm 4.9948 (6.1421)	loss_scale 65536.0000 (65536.0000)	mem 12648MB
[2025-05-23 13:38:51 vssm1_tiny_0230] (main.py 299): INFO Train: [0/300][90/195]	eta 0:03:13 lr 0.000006	 wd 0.0500	time 1.6396 (1.8470)	data time 0.0005 (0.2886)	model time 1.6390 (1.5035)	loss 5.2010 (6.1297)	grad_norm 4.5164 (5.9715)	loss_scale 65536.0000 (65536.0000)	mem 12648MB
[2025-05-23 13:39:06 vssm1_tiny_0230] (main.py 299): INFO Train: [0/300][100/195]	eta 0:02:52 lr 0.000007	 wd 0.0500	time 1.3098 (1.8110)	data time 0.0006 (0.2601)	model time 1.3092 (1.4993)	loss 5.1597 (6.0390)	grad_norm 4.1282 (5.8317)	loss_scale 65536.0000 (65536.0000)	mem 12648MB
[2025-05-23 13:39:21 vssm1_tiny_0230] (main.py 299): INFO Train: [0/300][110/195]	eta 0:02:31 lr 0.000007	 wd 0.0500	time 1.6545 (1.7852)	data time 0.0009 (0.2368)	model time 1.6535 (1.5035)	loss 5.2208 (5.9618)	grad_norm 4.4259 (5.7092)	loss_scale 65536.0000 (65536.0000)	mem 12648MB
[2025-05-23 13:39:36 vssm1_tiny_0230] (main.py 299): INFO Train: [0/300][120/195]	eta 0:02:12 lr 0.000008	 wd 0.0500	time 1.4918 (1.7615)	data time 0.0006 (0.2172)	model time 1.4912 (1.5027)	loss 5.0534 (5.8929)	grad_norm 4.5020 (5.5937)	loss_scale 65536.0000 (65536.0000)	mem 12648MB
[2025-05-23 13:39:51 vssm1_tiny_0230] (main.py 299): INFO Train: [0/300][130/195]	eta 0:01:53 lr 0.000009	 wd 0.0500	time 1.2720 (1.7401)	data time 0.0007 (0.2007)	model time 1.2713 (1.4998)	loss 5.1285 (5.8329)	grad_norm 3.8647 (5.4944)	loss_scale 65536.0000 (65536.0000)	mem 12648MB
[2025-05-23 13:40:03 vssm1_tiny_0230] (main.py 299): INFO Train: [0/300][140/195]	eta 0:01:33 lr 0.000009	 wd 0.0500	time 1.5417 (1.7017)	data time 0.0006 (0.1865)	model time 1.5411 (1.4663)	loss 5.0335 (5.7781)	grad_norm 4.8419 (5.4127)	loss_scale 65536.0000 (65536.0000)	mem 12648MB
[2025-05-23 13:40:18 vssm1_tiny_0230] (main.py 299): INFO Train: [0/300][150/195]	eta 0:01:15 lr 0.000010	 wd 0.0500	time 1.2936 (1.6871)	data time 0.0007 (0.1742)	model time 1.2928 (1.4677)	loss 4.9370 (5.7286)	grad_norm 4.8075 (5.3474)	loss_scale 65536.0000 (65536.0000)	mem 12648MB
